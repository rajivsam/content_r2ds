---
layout: default
title: Learning with Costs
parent: Algorithms
katex: true
---
## Description
Business or organizational actions taken on the basis of a prediction from a machine learning model have business or organizational consequences. For example, in a Banking application, rejecting a loan applicant who turns out to be credit worthy results in lost revenue to the Bank. Approving a loan to a defaulter, could result in a bigger loss. Excessive security redirections in a business website could be a detractor to potential customers. Lax security measures could result in legal charges and financial losses. Making decisions based on costa associated with actions is a critical need. {% cite DBLP_conf_ijcai_Elkan01 %} provides an excellent discussion of the basic concepts used in developing the theory around cost based decision making in machine learning applications. {% cite ling2008cost %} is a more recent (still over ten years old as of this writing) treatment that surveys approaches to cost based decision making. A key characteristic of making decisions where cost is relevant using supervised learning is that such applications are usually associated with _imbalanced_ datasets. Some classes of the target variable are _under-represented_ in the dataset. To make this idea concrete, consider the binary classification setting. There are two classes, one of which is _under-represented_. This is called the _minority class_. By contrast, the _over-represented_ class is called the _majority class_. This discussion follows the convention used in  {% cite DBLP_conf_ijcai_Elkan01 %}. The _minority class_ is associated with the $$ 1 $$ label and the _majority class_ is associated with the $$ 0 $$ label. In a balanced dataset, the prior probablity of the $$ 0 $$ and the $$ 1 $$ class is $$ 0.5 $$. A key idea used in solving the imbalance issue is to construct a balanced data set out of the original imbalanced dataset using sampling. {% cite DBLP_conf_ijcai_Elkan01 %} develops a theory that explains how we can create a balanced dataset out of an imbalanced dataset on the basis of costs associated with classifying an example with a particular target value ($$ 0 $$ or $$ 1 $$). {% cite hart1968condensed %} uses an idea based on developing a balanced dataset from the imbalanced data set by preserving the neighborhood of each minority class instance to be consistent with the neighborhood of the point in imbalanced raw dataset. See [this video link](https://www.youtube.com/watch?v=XRbb3IbUYK8&list=PLV7MgHu4-vg27CxbsIGu_1x6nN0PQL7_N&index=63) for more details on the idea. This principle is used developing the solution. This choice was made because {% cite DBLP_conf_ijcai_Elkan01 %} provides the rationale to undersample the minority class but does not prescribe _how_ to achieve it (fair enough). Random undersampling of the majority class can change the decision boundary that manifests in the original imbalanced dataset. Condensed nearest neighbors is attractive because it _preserves the decision boundary_ associated with problem. It should be noted that the probabilities predicated by the model developed on the data may still be noisy. Probability caliberation can be applied to correct noise. This will be addressed in a separate algorithmic recipe shortly.
{: style="text-align: justify"} 

## Algorithm Input Data Description
Costs associated with model decisions are inputs that are needed to apply this algorithm. These costs are developed based on Action Costs recipe. See [this notebook](https://github.com/rajivsam/learning_with_costs/blob/main/notebooks/preliminary_exploration_2010_7a_data.ipynb) for how false positive and false negative costs are estimated in a loan quality setting. The reduction of the imbalanced dataset to a balanced dataset using the techniques discussed in {% cite DBLP_conf_ijcai_Elkan01 %} and {% cite hart1968condensed %} are illustrated in [this notebook](https://github.com/rajivsam/learning_with_costs/blob/main/notebooks/model_selection.ipynb)
{: style="text-align: justify"} 

## Algorithm Solution Description

Feature engineering can be very useful in datasets with class imbalance. This turned out to be the case for the example used to illustrate the learning with costs algorithm. The example uses real world data published by the Small Business Adminsitration (SBA). It provides details about the loan repayment history for businesses it assists. Loan defaults and cancellations do occur, though most loans are paid in full. Predicting loan defaults is an established application in the finance domain. Location is important in real estate. It turned out to be pivot for feature engineering done with this dataset. Exploratory data analysis revealed that defaults occur only a specific set of zip codes. By creating a derived attribute that captures if a zip code associated with a loan falls witin this "bad" set of zip codes, we can compute a very effective loan quality model. Using just the raw features can lead you to models with poor performance. This underscores the value of exploratory data analysis, feature engineering and due dilligence in developing data science solutions. The solutions using the approach by {% cite DBLP_conf_ijcai_Elkan01 %} and {% cite hart1968condensed %} provide similar results. The [imbalanced-learn](https://imbalanced-learn.org/stable/) library was used for the reference implementation. In this recipe, the condensed nearest neighbor approach is used for constructing the balanced dataset, other approaches are also possible, see [imbalanced-learn](https://imbalanced-learn.org/stable/) for more details.
{: style="text-align: justify"} 

## Bibliography
{% bibliography --cited %}


 

